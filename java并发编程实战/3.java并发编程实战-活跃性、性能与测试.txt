活跃性、性能与测试
===============================================================================================
10.活跃性危险
	在安全性与活跃性之间通常存在着某种制衡。
	如果过度加锁，则可能导致锁顺序死锁(Lock-Ordering Deadlock)。同样，我们使用线程池和信号量来限制对资源的使用，但这些限制行为可能会导致资源死锁(Resource Deadlock)
	java应用程序无法从死锁中恢复过来，因此在设计时一定要排除那些可能导致死锁出现的条件
	你或许认为我有些夸大了死锁的风险，因为锁被持有的时间通常很短暂，然而在真实系统中，死锁往往都是很严重的问题。作为商业产品的应用程序每天可能要执行数十亿次获取锁-释放锁的操作。而只要在这数十亿次操作中有一次发生了错误，就可能导致程序发生死锁，并且即使是应用程序通过了压力测试也不一定会找出所有潜在的死锁。(具有讽刺意味的是，之所以短时间地持有锁，是为了降低锁的竞争程序，但却增加了在测试中找出潜在死锁风险的难度)。
	
	10.1 死锁 Deadlock
			每个人都拥有其他人需要的资源，同时又等待其他人已经拥有的资源，并且每个人在获得所有需要的资源之前都不会放弃已经拥有的资源。
			死锁有多种形式，当线程T1持有锁L1并想获得锁L2，而线程T2持有锁L2并尝试获得锁L1，那么这两个线程将永远地等待下去。这种情况就算最简单的死锁形式(又被称为"抱死" Deadly Embrace)。其中多个线程由于存在环路的锁依赖关系而永远等待下去。
			把每个线程假想为有向图中的一个节点，途中每条边表示的关系是：线程A等待线程B所占有的资源。一一画出后，如果在图中形成了一条环路，那么就存在一个死锁
			在DB的设计中考虑了监测死锁以及从死锁中恢复。在执行一个事务(Transaction)时可能需要获取多个锁，并一直持有这些锁直到事务提交。因此在两个事务之间很可能发生死锁，但事实上这种情况并不多见。如果没有外部干涉，那么这些事务将永远等待下去，但数据库不会让这种情况发生。当它检测到一组事务发生了死锁时(通过在表示等待关系的有向图中搜索循环)，将选择一个牺牲者并令其放弃这个事务。作为牺牲者的事务会释放它所持有的资源，从而使其它事务继续进行。数据库可以重新执行被强制终止的事务，而这个事务现在可以成功完成，因为所有跟它竞争资源的事务都已经完成了。
			JVM在解决死锁问题方面并没有DB那样强大。当一组java线程发生死锁时，游戏到处结束――这些线程永远不能再使用了。根据线程完成工作的不同，可能造成应用程序完全停止，或者某个特定的子系统停止，或者是性能降低。而恢复应用程序的唯一方式就算中止并重启它，并希望不要再发生同样的事情。
			与许多其它的并发危险一样，死锁造成的影响很少会立即显现出来。如果一个类可能发生死锁，那么并不意味着每次都会发生死锁，而只是表示有可能。当死锁出现时，往往是在最糟糕的时候――在高负载情况下。
			
		10.1.1 锁顺序死锁
			$参见图片和代码示例
			锁顺序死锁发生的原因是：两个线程试图以不同的顺序来获得相同的锁。如果按照固定的顺序来请求锁，那么就不会出现循环的加锁依赖性，因此也就不会发生锁顺序死锁
			想要验证锁顺序的一致性，需要对程序中的加锁行为进行全局分析。如果只是单独地分析每条获取多个锁的代码路径，那是不够的。例如代码实例中两个线程都采用了"合理的"方式来获得锁，它们只是不能互相兼容。
			
		10.1.2 动态的锁顺序死锁
			$参见图片和代码示例
			有时候，并能清楚地直到是否在锁顺序上有足够的控制权来避免死锁的发生。例如，有一个转账方法，它将资金从一个账户转入另一个账户。在开始转账之前，首先要获得这两个账户对象的锁，以确保通过原子方式来更新这两个账户中的余额,同时又不破坏一些不变性条件，例如"账户的余额不能为负数"
			解决此类锁顺序死锁问题的关键在于控制锁操作的顺序，并且保证在整个应用程序中都按照这个顺序来获取锁。
			在制定锁的顺序时，可以用System.identityHashCode()方法，该方法将返回由Object.hashCode()返回的值。虽然增加了一些新的代码，但却消除了发生死锁的可能性。在极少数情况下，两个对象可能拥有相同的散列值，此时必须通过某种任意的方法来决定这种情况下的锁顺序，为了避免这种情况，可以使用"加时赛"(TieBreaking)锁，在获取两个Account锁之前，首先获得这个"加时锁"，从而保证每次只有一个线程以未知的顺序获取这两个锁，从而消除了死锁发生的可能性。并且由于System.identityHashCode()中出现散列冲突的频率非常低，因此这项技术以最小的代价，换来了最大的安全性
			此外，如果Account中包含一个唯一的、不可变的、并且具备可比性的成员，例如帐号，那么要制定锁顺序就更加容易了:通过键值对对象进行排序，因而不需要使用"加时赛"锁。
		10.1.3 在协作对象之间发生的锁顺序死锁
			$参见图片和代码示例
			在锁顺序死锁或动态锁顺序死锁中，要查找死锁原因和代码是比较简单的，只需要找出那些需要获取两个锁的方法。然而有些情况下查找死锁则比较困难：如果在持有锁的情况下调用某个外部方法，那么就需要格外警惕死锁。
			如果在持有锁时调用某个外部方法，那么将可能出现活跃性问题。比如这个外部方法中可能会获取其它锁(这可能会产生死锁)，或者阻塞时间过长，导致其它线程无法及时获得当前被持有的锁。
		
		10.1.4 开放调用 Open Call
			方法调用相当于一种抽象屏障，因而你无须了解被调用方法中所执行的操作，况且你本来就应该知道。但也正是由于不知道在被调用方法中执行的操作，因此在持有锁的时候对调用某个外部方法将难以进行分析，从而可能出现死锁。
			如果在调用某个方法时不需要持有锁，那么这种调用被称为开放调用(Open Call)。
			依赖于开发调用的类通常能表现出更好的行为，并且与那些在调用方法时需要持有锁的类相比，也更易于编写。这种通过开放调用来避免死锁的方法，类似于采用封装机制来提供线程安全的方法：虽然在没有封装的情况下也能确保构建线程安全的程序，但对一个使用了封装的程序进行线程安全分析，要比分析没有使用封装的程序容易得多。同理，分析一个完全依赖于开发调用的程序的活跃性，要比分析那些不依赖开放调用的程序的活跃性简单。通过尽可能地使用开发调用，将更易于找出那些需要获取多个锁的代码路径，因此也就更容易确保采用一致性的顺序来获得锁。
			在程序中应该尽量使用开发调用。与那些在持有锁时调用外部方法的程序相比，更易于对依赖于开放调用的程序进行死锁分析。
			有时候，在重写编写同步代码块以使用开发调用时会产生意想不到的结果，因为这会使得某个原子操作变为非原子操作。在许多情况下，使某个操作失去原子性是可以接受的。例如对于两个操作：更新出租车位置以及通知调度程序这两出租车已准备好出发去一个新的目的地，这两个操作并不需要为实现为一个原子操作。在其它情况中，虽然去掉原子性可能会出现一些值得注意的结果，但这种语义变化仍然是可以接受的。
			然而，在某些情况下，丢失原子性会引发错误，此时需要通过另一种技术来实现原子性(非同步锁)。例如在构造一个并发对象时，使得每次只有单个线程执行使用了开发调用的代码路径。
		10.1.5 资源死锁
			正如当多个线程相互持有彼此正在等待的锁而又不释放自己已持有的锁时会发生死锁，当它们在相同的资源集合上等待时，也会发生死锁。
			资源死锁情景一：假设有两个资源池，例如两个不同DB的连接池。如果一个任务需要连接两个DB，并且在请求这两个资源时不会始终遵循相同的顺序，那么线程A可能持有与DB1的连接，并等待与DB2的连接，而线程B则持有与DB2的连接并等待与DB2的连接。(资源池越大，出现这种情况的可能性越小。如果每个资源池都有N个连接，那么在发生死锁时不仅需要N个循环等待的线程，而且还需要大量不恰当的执行时序。)
			资源死锁情景二：另一种基于资源的死锁形式就算线程饥饿死锁(Thread-Starvation Deadlock)。例如一个单线程的Executor线程池中，一个任务提交另一个任务并等待Executor执行完成时可能造成的资源死锁。如果某些任务需要等待其它任务的结果，那么这些任务往往是产生线程饥饿死锁的主要来源，有界线程池/资源池与相互依赖的任务不能一起使用。
		10.2 死锁的避免与诊断
			如果一个程序每次至多只能获得一个锁，那么就不会产生锁顺序死锁。当然，这种情况通常并不现实。如果必须获得多个锁，那么在设计时必须考虑锁的顺序：尽量减少潜在的加锁交互数量，将获取锁时需要遵循的协议写入正式文档并始终遵循这些协议。
			可以通过使用一种两阶段策略(Two-Part Strategy)来检查代码中的死锁：
				首先，找出在什么地方将获取多个锁(使这个集合尽可能小)
				然后对所有这些实例进行全局分析，从而确保它们在整个程序中获取锁的顺序都保持一致。
				尽可能地使用开发调用，这能极大地简化分析过程。如果所有的调用都是开放调用，那么要发现获取多个锁的实例是非常简单的，可以通过代码审查，或者借助自动化的源代码分析工具。
			10.2.1 通过定时锁解决死锁问题
				显式锁Lock类中的定时tryLock功能可以用于代替内置锁机制。此外显式锁可以指定一个超时时限(Timeout)，在等待超过该时间后tryLock会返回一个失败信息。如果超时时限比获取锁的时间要长很多，那么就可以在发生某个意外情况后重新获得控制权。
				当定时锁失败时，你并不需要知道失败的原因。或许是因为发生了死锁，或许某个线程在持有锁时错误地进入了无限循环，还可能是某个操作的执行时间远远超过了你的预期。然而，至少你能记录发生的失败数，以及关于这次操作的其它有用信息，并通过一种更平缓的方式来重新开始计算，而不是关闭整个进程。
				即使在整个系统中没有始终使用定时锁，使用定时锁来获取多个锁也能有效地应对死锁问题。如果在获取锁时超时，那么可以释放这个锁，然后后退并在一段时间后再次尝试，从而消除了死锁发生的条件，使程序恢复过来。(注意，这种技术只有在同时获得两个锁时才有效，如果在嵌套的方法调用中请求多个锁，那么即使你直到已经持有了外层的锁，也无法释放它)
			10.2.2 线程转储(Thread Dump)分析死锁
				JVM提供了线程转储(Thread Dump)来帮助识别死锁的发生。线程转储是指JVM中在某一给定的时刻运行的所有线程的快照。线程转储信息中包括各个运行中的线程的栈追踪信息，这类似于发生异常时的栈追踪信息。
				线程转储还包含了加锁信息，例如每个线程持有了哪些锁，在哪些栈帧中获得了这些锁，以及被阻塞的线程正在等待获取哪一个锁。在生成线程转储之前，JVM将在等待关系图中通过搜索循环来找出死锁。如果发现了一个死锁，则获取相应的死锁信息，例如在死锁中涉及了哪些锁和线程，以及这个锁的获取操作位于程序的哪些位置。
				即使没有发生死锁，这些信息对于调试和程序监控来说也是有用的。通过定期触发线程转储，可以观察程序的加锁行为。
				使用显式锁Lock和内部所虽然都支持线程转储和死锁检测，但在显式锁上获得的信息比内置锁上获得的信息精确度低。内置锁与获得它们所在的线程栈帧是相关联的，而显式Lock只与获得它的线程相关联。
		
		10.3 其它活跃性危险
			虽然死锁是最常见的活跃性危险，但在并发程序中还存在一些其它的活跃性危险，包括：饥饿、丢失信号、活锁等。
			10.3.1 饥饿Starvation：
				当线程由于无法访问它所需要的资源而不能继续执行时，就发生了饥饿。引发饥饿的最常见资源就是CPU时钟周期。如果在java应用程序对线程的优先级使用不当，或者在持有锁时进行一些无法结束的结构(如无限循环，或者无限制地等待某个资源)，那么也可能导致饥饿，因为其它需要这个锁的线程将无法得到它。
				OS的线程调度器会尽力提供公平的、活跃性良好的调度，甚至超出java规范的需求范围。线程优先级并不是一种直观的机制，而通过修改线程优先级所带来的效果通常也不明显。当提高某个线程的优先级时，可能不会起到任何作用，或者也可能使得某个线程的调度优先级低于其它线程，从而导致饥饿。
				java中的线程优先级默认为5，但常，我们尽量不要改变线程的优先级。只要改变了线程的优先级，程序的行为将与平台相关而非jvm，并且会导致发生饥饿问题的风险。如果你经常能在某个程序的一些奇怪的地方调用Thread.sleep或Thread.yield，这大概是因为该程序试图克服优先级调整问题或响应性问题，并试图让低优先级的线程执行更多的时间。
				要避免使用线程的优先级，因为这会增加平台依赖性从而影响java程序的移植性，并可能导致活跃性问题。在大多数并发应用程序中，都可以使用默认的线程优先级。
			10.3.2 糟糕的响应性：
				对于java的GUI应用程序，如果触发事件后在后台是CPU密集型任务，则可能会对程序响应性造成影响，因为它们会与事件线程竞争CPU时间片。
				不良的锁管理也可能导致糟糕的响应性。如果某个线程长时间占有一个锁(如对大容器迭代，或进行计算密集型处理)，这时其它想要访问这个容器的线程就必须等待很长时间。
			10.3.3 活锁(LiveLock)：
				当多个相互协作的线程都对彼此进行响应从而修改各自的状态，并使得任何一个线程都无法继续执行时，就发生了活锁。这就好比两个过于礼貌的人在半路上面对面地相遇：他们彼此都让出对方的路，然而又在另一条路上相遇了。因此他们就这样反复地避让下去。
				活锁问题尽管不会阻塞线程，但它也无法继续执行，因为线程将不断重复执行相同的操作，并且总会失败。
				活锁的解决需要在重试机制中引入随机性。例如网络上如果两台及其尝试使用相同的载波发送数据包，那么这些数据包就会发生冲突。这两台机器都检查到了冲突，并都在稍后再次重发。如果两者都选择了在1s后重试，那么它们又会发生冲突，并且不断地冲突下去，因此即使有大量闲置的带宽，也无法使数据包发送出去。为了避免这种情况发生，需要让它们分别等待一段随机时间。在并发应用中，通过等待随机时长和回退可以有效地避免活锁的发生

11.性能与可伸缩性：
	多线程的目的是提高程序的运行性能。线程可以使程序更加充分地发挥系统的可用处理能力，从而提高系统的资源利用率。此外线程还可以使程序在已有任务运行的情况下立即开始处理新的任务，从而提高系统的响应性。
	虽然我们希望获得更好的性能，但无论如何，始终要把安全性放在第一位。首先要保证程序能正确运行，然后仅当程序的性能需求和测试结果要求程序执行得更快时，才应该设法提高它的运行速度。
	然而，虽然许多提升性能的技术同样会增加复杂性，因此也就增加了代码在安全性和活跃性上失败的风险。更糟糕的是，虽然某些技术初衷是提升性能，但事实上却与最初的目标背道而驰，或者又带来了其它新的性能问题。本章将介绍各种分析、检测及提升并发程序性能的技术。
	11.1 对性能的思考：
		提升性能意味着用更少的资源做更多的事情。
		11.1.1 性能与可伸缩性：
			应用程序的性能可以采用多个指标来衡量，例如服务时间、延迟时间、吞吐率、效率、可伸缩性以及容量等。其中一些指标(服务时间、等待时间)用于衡量程序的运行速度，即某个指定的任务单元需要多快才能处理完成。另一些指标(生产量、吞吐量)用于程序的处理能力，即在计算资源一定的情况下，能完成多少工作。
			可伸缩性：当增加计算资源时(如CPU、内存、硬盘或IO带宽)，程序的吞吐量或者处理能力相应地增加。
			在并发应用程序中针对可伸缩性进行设计和调整时所采用的方法与传统的性能调优方法截然不同。当进行性能调优时，其目的通常是用更小的代价完成相同的工作，例如通过缓存重用之前计算的结果，或者通过更优的算法来降低时间复杂度的大O值。而在进行可伸缩性调优时，其目的是设法将问题的计算并行化，从而能利用更多的计算资源来完成更多的工作。
			性能的这两个方面――“多快”和“多少”，是完全独立的，有时候甚至是相互矛盾的。要实现更高的可伸缩性或硬件利用率，通常会增加各个任务所要处理的工作量，例如把任务分解为多个“流水线”子任务时。具有讽刺意味的是，大多数提高单线程程序性能的技术，往往都会破坏可伸缩性。
			单一的应用程序避免了在不同层次之间传递任务时存在的网络延迟，同时也不需要将计算过程分解到不同的抽象层次，因此能减少许多开销(例如在任务排队、线程协调以及数据复制时存在的开销)。然而，当这种单一的系统达到自身处理能力的极限时，会遇到一个严重的问题：要进一步提升它的处理能力将非常困难。因此，我们通常会接受每个工作单元执行更长的时间或消耗更多的计算资源，以换取应用程序在增加更多资源的情况下处理更高的负载。
			对于服务器应用程序来说，“多少”这个方面――可伸缩性、吞吐量和生产量，往往比“多快”这个方面更受重视。(在交互式应用程序中，延迟或许更加重要，这样用户就不用等待进度条的指定，并奇怪程序究竟在执行哪些操作)
		11.1.2 评估各种性能权衡因素
			避免不成熟的优化，首先应保证程序正确，然后再提高运行速度――如果它还运行得不够快。而通常越快的算法越复杂
			免费的perfbar可以给出CPU的忙碌程序信息，而我们通常的目标就是使CPU保持忙碌状态。
	11.2 Amdahl定律：
		在有些问题中，如果可用资源越多，那么问题的解决速度就越快。例如，如果参与收割庄稼的农民越多，那么就能越快地完成收割工作。而有些任务本质上是串行的，例如庄稼的生长速度，即使增加再多的工人也不可能增加作物的生长速度。如果使用线程主要是为了发挥多个处理器的处理能力，那么就必须对问题进行合理的并行分解，使得程序能有效地使用这种潜在的并行能力。大多数并发程序都与农业耕作类似，它们都是由一系列的并行工作和串行工作所组成。
		Amdahl：在增加计算资源的情况下，程序在理论上能够实现最高加速比，这个值取决于程序中可并行组件与串行组件所占的比重。随着处理器数量的增加，即使串行部分所占比例很小，也能极大地限制当增加计算资源时能够提升的吞吐率。
		
		11.2.1 代码中隐藏的串行部分
			要预测应用程序在某个多处理器系统中将实现多大的加速比，还需要找出任务中的串行部分。
			在所有并发程序中都包含一些串行部分，如果你任务在你的程序中不存在串行部分，那么你可以再仔细检查一遍。
		11.2.2 Amdahl定律的应用
			如果能准确估算出执行过程中串行部分所占比例，那么Amdahl定律就能量化当有更多CPU可用时的加速比。虽然要直接测量串行部分的比例非常困难，但即使不进行测试的情况下Amdahl定律仍然是有用的。
			随着多核CPU逐渐成为主流，系统可能拥有几十上百甚至数千个CPU，一些在4核系统中砍死具有可伸缩性的算法，却可能含有一些隐藏的可伸缩性的瓶颈，只是还没有遇到而已。在评估一个算法时，要考虑算法在数百个或数千个CPU的情况下的性能表现，从而对可能出现的可伸缩性局限有一定程度的认识。例如锁分解(将一个锁分解为两个锁，如读写锁的分离)和锁分段(将一个锁分解为多个锁)。就性能的扩展性来说，锁分段技术更优前途，因为分段的数量可以随着处理器数量的增加而增加。
	11.3 线程引入的开销:
		单线程程序既不存在线程调度，也不存在同步开销，而且不需要使用锁来保证数据结构的一致性。
		在多个线程的调度和协调过程中都需要额外的性能开销，因此为了提升性能而引入的线程来说，并行带来的性能提升必须超过并发导致的开销。
		11.3.1 上下文切换:
			OS每次调度线程，会导致一次上下文切换，在这个过程中将保存当前运行线程的执行上下文，并将新调度进来的线程的执行上下文设置为当前上下文。
			切换上下文需要一定的开销，而在线程调度过程中需要访问由OS和jvm共享的数据结构。但线程上下文切换的开销并不只是包含jvm和OS的开销。但一个新的线程被切换进来时，它所需要的数据可能不在当前CPU的本地缓存中，因此上下文切换会导致一些缓存缺失，因而读取这些信息会导致线程在首次调度运行时会更加缓慢。这也是为什么调度器会为每个可运行的线程分配一个最小执行时间。它实际上是将上下文切换的开销分摊到更多不会中断的执行时间上，从而提高整体的吞吐量。(以损失响应性为代价)
			当线程由于等待某个发生竞争的锁而被阻塞时，jvm通常会将这个线程挂起，并允许它被交换出去。如果线程频繁地发生阻塞，那么它们将无法使用完整的调度时间片。在程序中发生越多的线程阻塞(如IO阻塞，同步锁阻塞，在变量条件上等待等)，与CPU密集型的程序就会发生越多的上下文切换，从而增加调度开销，并因此而降低吞吐量。
			上下文切换的实际开销随OS和CPU的不同而变化，按照经验来看，大多数通用的处理器中，上下文切换的开销相当于5000~10000个时钟周期，大约是几微秒。
			应用程序，jvm以及OS都使用一组CPU，因此在jvm和OS的代码中消耗的CPU时钟周期越多，应用程序的可用CPU时钟周期就越少。如果OS占用率较高(超过10%)那么通常表示调度活动发生频繁，这很可能是由于IO或竞争锁导致的阻塞而引起的。vmstat和pidstat查看
		11.3.2 内存同步：
			同步操作的性能开销包括多个方面，除了CPU的上下文切换外，内存可见性保证中也有一定的开销。
			在Synchronized和volatile提供的可见性保证中可能会使用一些特殊指令:内存栅栏(Memory Barrier)。内存栅栏可以刷新缓存，使缓存无效，刷新硬件的写缓冲，以及停止执行管道。内存栅栏可能同样会对性能带来间接的影响，因为它们将抑制一些编译器优化操作。在内存栅栏中，大多数操作都是不可能被重排序的。
			不要过度担心非竞争同步带来的开销。这个基本的机制已经非常快了，并且jvm还能进行额外的优化以进一步降低或消除开销。因此，我们应该将优化重点放在那些发生锁竞争的地方。
			某个线程中的同步可能会影响其它线程的性能。同步会增加共享内存总线上的通信量，总线的带宽是有限的，并且所有的处理器都将共享这条总线。如果有多个线程竞争同步带宽，那么所有使用了同步的线程都会受到影响。
		11.3.3 阻塞与自旋锁：
			非竞争的同步可以完全在jvm中进行处理，而jvm会对这种非竞争的同步锁进行优化，如锁消除、锁粗化等。
			而有竞争的同步可能需要OS的介入，从而增加开销。当在锁上发生竞争时，竞争失败的线程肯定会阻塞。而jvm在处理阻塞行为时，可以采用自旋等待(Spin-Waiting,指通过循环不断地尝试获取锁，直到成功)，或者通过OS来挂起被阻塞的线程。这两种方式的效率高低，要取决于上下文切换的开销以及在成功获取锁之前需要等待的时间。如果等待时间较短，则适合采用自旋等待的方式，而如果等待时间较长，则适合采用线程挂起方式。默认情况下大多数jvm都是在等待锁时会将线程挂起(视jvm版本而定)。
			当线程无法获取某个锁或者由于在某个条件等待或在IO操作上阻塞时，需要被挂起，在这个过程中将包含两次额外的上下文切换，以及所有必要的OS操作和缓存操作：被阻塞的线程在其执行时间片还未用完之前就被交换出去，而在随后当要获取的锁或者其它资源可用时，又再次被切换回来(由于锁竞争而导致阻塞时，线程在持有锁时将存在一定的开销：当它释放锁时，必须告诉操作系统恢复运行阻塞的线程)。
	11.4 减少锁的竞争
		在对由某个独占锁保护的资源进行访问时，将采用串行方式――每次只有一个线程访问它。这样能够保证数据不会被破坏，但获得这种安全性是要付出代价的。如果在锁上持续发生竞争，那么将限制代码的可伸缩性。
		在并发程序中，对可伸缩性的最主要威胁就算独占式的资源锁。
		有两个因素将影响在锁上发生竞争的可能性：锁的请求频率，以及每次持有该锁的时间。如果两者的乘积很小，那么大多数获取锁的操作都不会发生竞争，因此在该锁上的竞争不会对可伸缩性造成严重影响。然而，如果在锁上的请求量很高，那么需要获取该锁的线程将被阻塞并等待。在极端情况下，即使仍有大量工作等待完成，处理器也会被闲置。
		有3种方式可以降低锁的竞争程度：
			1.减少锁的持有时间。
			2.减少锁的请求频率。
			3.使用带有协调机制的独占锁，这些机制运行更高的并发性。
		11.4.1 缩小锁的范围("快进快出")
			降低放生竞争可能性的一种有效方式就算尽可能缩短锁的持有时间。因为锁被持有的时间越长，那么在这个锁上发生竞争的可能性就越大。
			如果将一个"高度竞争"的锁持有过长的时间那么会限制可伸缩性，例如如果某个操作持有锁的时间超过2ms并且所有操作都需要这个锁，那么无论用于多少个CUP，吞吐量也不会超过每秒500个操作。而如果将这个锁的持有时间将为1ms，那么能够将这个锁对应的吞吐量提高到每秒1000个操作。
			使用更细粒度的Synchronized块来减小锁的作用范围，能极大地减少在持有锁时需要执行的指令数量。根据Amdahl定律，这样消除了限制可伸缩性的一个因素，因为串行代码的总量减少了。
			尽管缩小同步代码块能提高可伸缩性，但同步代码块也不能过小――――一些需要采用原子方式执行的操作(例如对某个不变性条件中的多个变量进行更新)必须包含在一个同步块中。此外，同步需要一定的开销，当把一个同步代码块分解为多个同步代码块时(在确保正确性的情况下)，反而会对性能提升产生负面影响。在分解同步代码块时，理想的平衡点将与平台相关，但在实际情况中吗，仅当可以将一些"大量"的计算或阻塞操作从同步代码块中移出时，才应该考虑优化同步代码块的大小。
		11.4.2 减小锁的粒度(锁分解、锁分段)：
			另一种减小锁的持有时间的方式是降低线程请求锁的频率(从而减小发生竞争的可能性)。这可以通过锁分解和锁分段等技术来实现，在这些技术中将采用多个互相独立的锁来保护独立的状态变量，从而改变这些变量在之前由单个锁来保护的情况。这些技术能减小锁操作的粒度，并能实现更高的可伸缩性，然而，使用的锁越多，那么发生死锁的风险也就越高。
			如果一个锁需要保护多个相互独立的状态变量，那么可以将这个锁分解为多个锁，并且每个锁只保护一个变量，从而提高可伸缩性，从而最终实现降低每个锁被请求的频率。
			如果在锁上存在适中而不是激烈的竞争时，通过将一个锁分解为两个锁，能最大限度地提升性能。如果对竞争并不激烈的锁进行分解，那么在性能和吞吐量等方面带来的提升将非常有限，但是也会提高性能随着竞争提高而下降的拐点值。对竞争适中的锁进行分解时，实际上是把这些锁转变为非竞争的锁，从而有效地提高性能和可伸缩性。
		11.4.3 锁分段：
			如果一个锁竞争激烈，将它分解为两个锁时，这两个锁可能依然都存在激烈的竞争。虽然采用两个线程并发执行能提高一部分伸缩性，但在一个拥有多个处理器的系统中，仍然无法给可伸缩性带来极大的提高。
			某些情况下，可以将锁分解技术进一步扩展为一组独立对象上的锁进行分解，这种情况被称为锁分段。
			例如ConcurrentHashMap的实现中使用了一个包含16个锁的数组，每个锁保护所有散列桶的1/16，其中第N个散列桶由第(N mod 16)个锁来保护。假设散列函数有合理的分布，那么大约能把对于锁的请求减少到原来的1/16。因此这项技术使得ConcurrentHashMap能够支持多达16个并发的写入器。当然，默认的锁分段数量也可以自己指定，最大30。
			锁分段的一个劣势在于：与采用单个锁来实现独占访问相比，要获取多个锁来实现独占访问将更加困难并且开销更高，因此清除容器的接口实现开销较大。
		11.4.4 避免热点域：
			锁分解和锁分段技术都能提高可伸缩性，因为它们都能在不同的线程在不同的数据或者同一数据的不同部分上操作，且不会相互干扰。
			但是当每个操作都请求多个变量时，锁的粒度将很难降低，因为需要对多个变量进行同步策略控制。这是在性能与可伸缩性之间相互制衡的另一个方面，一些常见的优化措施，例如将一些反复计算的结果缓存起来，都会引入一些"热点域"(Hot Field)，而这些热点域往往会限制可伸缩性。
			HashMap的实现时，你需要考虑如何在size方法中计算Map中的元素数量。最简单的方法就是，在每次调用时都统计一次元素数量。一种常见的优化措施是，在插入和移除元素时更新一个计数器，虽然这在put和remove方法中略微增加了一些开销，以确保计数器是最新的值，但这将把size方法的开销从O(n)降到O(1)。
			HashMap是在单线程或者完全同步的实现中，使用一个独立的计数能很好地提高类似size和isEmpty这些方法的执行速度，但却导致更难以提升实现的可伸缩性，因为每个修改map的操作都需要更新这个共享计数器。即使使用锁分段技术来实现散列链，那么在对计数器的访问进行同步时，也会重新导致在使用独占锁时存在的可伸缩性问题。一个看似性能优化的措施――缓存size操作的结果，却变成了一个可伸缩性的障碍，在这种情况下，计数器也被称为热点域，因为这会导致每次元素数量发生变化的操作都需要对它进行操作。
			为了解决热点问题，ConcurrentHashMap中的size将对每个分段进行枚举并将每个分段中的元素数量相加，而不是维护一个全局计数。而为了避免枚举分段中的每个元素，ConcurrentHashMap为每个分段都维护了一个独立的计数,并通过每个分段的锁来维护这个值。
		11.4.5 使用替代独占锁的方法：
			最好的降低锁竞争的技术就算放弃使用同步锁，使用一种更为并发友好的方式来管理共享状态。例如，使用并发容器、读-写锁、不可变对象、原子变量等。
	11.5 减少因IO造成的上下文切换的开销：
		在许多任务中都包含一些可能被阻塞的操作。当任务在运行和阻塞这两个状态之间转换时，就相当于一次上下文切换。在服务器应用程序中，发生阻塞的原因之一就是在处理请求时产生各种日志消息。大多数日志框架中都是简单地对println进行包装，当需要记录某个消息时，只需要将其写入日志文件中。本书中给出另一种日志处理方式LogWriter:记录日志的工作专门由一个后台线程完成，而不是由发出请求的线程完成。这两种方法基本上是相同的，但二者性能上会存在一些差异。这取决于多少线程正在记录日志以及上下文切换的开销等。
		通过将IO操作从处理请求的线程中分离出来，可以缩短处理请求的平均服务时间。调用log方法的线程将不会再因为等待输出流的锁或者IO完成而被阻塞，它们只需要将消息放入队列，然后就返回到各自的任务中。从某种意义上讲，我们只是将工作分散开来，并把IO操作移到了另一个用户感知不到开销的线程上。通过把所有记录日志的IO转移到一个线程，还消除了输出流上的竞争。这将提升整体的吞吐量，因为在线程调度中上下文切换次数更少，并且锁的管理 也更简单。

总结：
	由于使用线程常常是为了充分利用多个处理器的计算能力，因此在并发程序性能的讨论中，通常更多地将则重点放在吞吐量和可伸缩性上，而不是服务时间。Amdahl定律告诉我们，程序的可伸缩性取决于在所有代码中必须被串行执行的代码比例。因为java程序中串行操作的主要来源是独占方式的资源锁，因此通常可以通过一下方式来提升可伸缩性：减少锁的持有时间，降低锁的粒度，以及采用非独占的锁或非阻塞锁来代替独占锁。
	
------------------------------------------------------------------------------------------------------------------------					
12.并发程序的测试：
	在测试并发程序时，所面临的挑战在于：潜在错误的发生并不具有确定性，而是随机的。要在测试中将这些故障暴露出来，就需要比普通的串行程序测试覆盖更广的范围并且执行更长的时间。
	并发测试大致分为两类，即安全性测试和活跃性测试。我们将安全性测试定义为"始终正确，不发生任何错误的行为"，而将活跃性定义为"某个良好的行为终究会发生"。
	测试活跃性本身也存在问题。活跃性测试包括进展测试和无进展测试两方面，这些都是很难量化的――如何验证某个方法是被阻塞了，而不只是运行缓慢？同样，如何测试某个算法不会发生死锁？要等多久才能宣告它发生了故障？与活跃性测试相关的是性能测试。性能可以通过多个方面来衡量，包括；
		吞吐量：指一组并发任务中已完成任务所占的比例。
		响应性：指请求从发出到完成之间的时间(也称延迟)。
		可伸缩性：指在增加更多资源的情况下(通常指CPU)对吞吐量的提升情况。
		
	
	
